\section{Experimental Evaluation}\label{results}

% No attack was able to leak, but number of false positive increased. 



Accuracy on 3-fold cross validation improved by 7\%


We take an attack sample first and project it back to the latent space $z$.
We use the generator $G$ to generate a similar example to $x$, called ${x^{\star}}$ by $G(z)$. Then we use the classifier $C$ to classify the example $C(x^{\star})$, which generally already tends to misclassify way less than running the classification directly on $x$. 

Performance of the new classifier does drop on the training set, but accuracy improves on test set including adversarial attacks. Figure shows APL achieves significant improvement in classification accuracy as the number of training examples increases, especially synthetic examples produced by DCGAN. The dotted line depicts classification performance for classic data augmentation using program synthesizers. The performance improves as the quantity of new (augmented) training examples increases; however, the improvement plateaus around the accuracy of 80\%, beyond which additional examples fail to yield improvement. 

The dashed line shows the additional increase in accuracy achieved by augmenting the dataset using GAN-produced synthetic examples. Starting from the point beyond which additional classically augmented examples stopped improving accuracy, we added syntetic data generated by our GAN. The classification performance improved from 83\% to pver 90\%, demonstrating the usefulness of \scheme. 

Figure shows the loss value of generator vs training epoch for a range of features. Using a few high level features would result in discriminator losing the game to the generator faster. we define the new metric {\em Nash Equilibrium Speed} to be the loss of generative model over 1000 epochs. that is proportional to the vulnerability to adversarial machine learning attack and  inversely proportional  to the resiliency of a model to adversarial machine learning attacks. 

People have noted that adding labels to the dataâ€”that is, to break it up into categories, almost always improves the performance of GANs.

Generated data, showing high features in .. which is high in all the variations. 


