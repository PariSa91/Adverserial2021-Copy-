\section{ Overview}\label{overview}
\subsection{General Approach}
%\subsection{Game setup}

 The microarchitectural attack detection model {\em e.i.,} PerSpectron, learns the patterns of suspicious and safe activity in a program to discern an attacks footprints.
When generating handcrafted adversarial attacks, our goal was to produce a program that capture the characteristics of the training dataset {\em i.e., } of safe programs in hardware, so that the attacks it generates look indistinguishable from the training safe data in the eyes of PerSpectron. Thus our goals as an attacker can be thought of as PerSpectron's goal in reverse. That implies that there is game setup between our model's goal and the attackers. Our design is inspired by Generative Modeling Theory specifically GANS~\cite{goodfellow2014generative}.We first discuss the theory behind GANs and then describe our design.  


\subsection{Generative Modeling Theory}
A generative model describes how a dataset is generated in
terms of a probabilistic model. By sampling from the model,
we are able to generate new data. GAN is a framework for
estimating generative models via an adversarial process, in
which it simultaneously train two models: a generative model
that captures the data distribution, and a discriminator model
that estimates the probability that a sample came from the
training data rather than the generator. The training procedure
for generator is to maximize the probability of discriminator
making a mistake~\cite{goodfellow2014generative}.


Instead of recognising the pattern, the Generator learns to create them essentially from scratch; indeed, the input into the Generator is often no more than a vector of random numbers.  The Generator learns through the feedback it receives from the discriminator's classification. The discriminator's goal is to determine whether a particular example is coming from the training dataset or created by the Generator.
 
 
Accordingly, each time the discriminator is fooled into classifying an adversarial attack as safe, the generator knows it did something well. Conversely each time the discriminator correctly rejects a generator-produced adversarial as attack, the generator receives the feedback that it needs to improve.
The discriminator continues to improve as well. Like any classifier, it learns from how far its predictions are from the true class (safe or suspicious). So, as the generator gets better at producing realistic adversarial sample, the discriminator gets better at telling adversarial samples from safe programs data, and both models continue to improve simultaneously.

% We have a theoretical understanding of why the symmetric GAN training should converge to the Nash equilibrium. We noticed that our asymmetric GAN traning has a lot higher gradient and so trianing happens much more quickly at start. Although theoretically there is a chance that the training might not converge at all, we empirically show that with the right stopping criteria, the trained model performs better than symmetric GAN training. 
% But our dreadful sacrifice leads to significant improvement in accuracy.


\subsection{}
%A conditional GAN allows us to direct the generator to synthesize the adversarial example we want.


Although we could control the domain of example our GAN learned to emulate by our selection of the training dataset, we could not specify any of the characteristics of the data samples it is going to generate. GAN could synthesize realistic looking handwritten digits but we can not control what digits it produces.




 \subsection{Training with Data Transmit And Recovery Channel Control}
 GANs are capable of producing examples ranging from simple handwritten digits to photo-realistic images of human faces. However, although we could control the domain of examples our GAN learned to emulate by our selection of the training dataset. we could not specify any of the characteristics of the data samples the gan would generate. We could not control whether it would produce, say a new sample of adversarial meltdown attack. In image recognition, this concern may seem trivial. Because if the goal is to generate number 9, you can just keep generating until you get the number you want. However, for a domain of microarchitectural attacks the domain of possible answers gets too large for such a brute-force solution to be practical. In simple GAN, you have no control on what category of sample input will get produced. There is no way to direct the Generator to synthesize say an attack or safe program sample. Let alone other features such as the type of covert channel such as Flush+Reload or Flush+Flush.
 
 Our design has the ability to decide what kind of adversarial microarchitectural attack will be generated. We could enter the descriptive features of attacks atomic tasks' samples into the generator and have it output a range of samples matching the criteria. It can greatly expedite the process of adversarial attack generation. We are sure there are many other practical applications where the ability to generate new attack sample that matches the channel type of our choice would a game changer. The CGAN was one of the first GAN innovations that made data generation possible. 
 
 CGAN is a generative adversarial learning whose generative and discriminator are conditioned during training by using some additional information e.g., labels. 
 The generator learns to produce realistic examples for each label in the training dataset and the discriminator learns to distinguish fake example-label pair from real-example pairs.   
 in contrast to a design where the Disriminator learns to assign a correct label to each real example in addition to distinguashing real example from fake, the discriminator does not learn to identify which class is which.  It learns to accept real matching pair while rejecting pairs that are mistmatched and pairs in which there are fake. 
 
 Accordingly in order to fool the discriminator it is not enough for the CGAN generator to produce realistic looking data. The example it generates also need to match their labels. After the generator is fully trained, this then allows us to specify what attack type we want the CGAN to synthesize by passing it the desired label. 
 
 The generator uses the noise vector and label to synthesize a fake example (x given that or conditioned on y). The goal of this fake example is to look in the eyes of Discriminator as close as possible to a real example for the given label.  
 
 
 
 The discriminator receives real examples with labels and fake examples with the label used to generate them. on the real example-label pairs, the discriminator learns how to recognize real data and how to recognize matching pairs. On the generator-based examples, it learns to recognize fake program-labeled pairs, thereby learning to tell them apart from the real ones. The discriminator outputs a single probability indicating its conviction that input is a real, matching pair. The discriminator's goal is to learn to reject all fake examples and all examples that fail to match their label, while accepting all real example-label pairs. 
 
 Note that for each fake example the same label is passed to both the generator and discriminator. Also note that the discriminator is never explicitly trained to reject mismatched pairs by being trained on real examples with mismatching labels; its ability to identify mismatched pairs is a by-product of being trained to accept only real matching pairs. 
 

\subsection{Training}
GANs are notoriously hard to both train and evaluate. As with any other cutting-edge field, opinions about what is the best approach are evolving and no study has been done on microarchitectural data. 

We want to use GANs to create a large dataset, but we need a large dataset to train the GAN in the first place. 
Our solution is as follows:
first we use standard data augmentation using program synthesising to create a larger dataset. Second, we used this dataset to train a GAN to create synthetic examples. Third, we use the augmented dataset from step 1 along with the GAN-produced synthetic examples from step 2 to train the PerSpectron. The GAN model we used is a variation of Deep Convolutions GAN. We had to adjust the dimensions of hidden layers and the dimensions of the output from the Generator and input into the Discriminator PerSpectron. Then we used the trained PerSpectron to classify unseen test set.  

Training dataset-- The dataset of all safe programs and real attacks that we want the generator to learn to emulate with near-perfect quality. This dataset serves as input($x$) to the generator network. 

Random noise vector-- The raw input ($z$) to the Generator network. This input is a vector of random numbers that the generator uses as a starting point for generating adversarial examples. 

Discriminator network-- The discriminator takes as input either the real examples ($x$) coming from the training set or an adversarial example $x^{\star}$ produced by the generator. For each example, the discriminator determines and outputs the probability of whether the example is adversarial.

Iterative training/tuning--For each of the discriminators predictions, we determine how good it is--much as we would for a regular classifier-- and use results to attractively tune the discriminator and the generator networks through backpropogation; the discriminator's weights and biases are updated to maximize its classification and accuracy ( maximizing the probability of correct prediction: ($x$) as attack and $x^{\star}$ is safe. 
The generator's weights and biases are updated to maximize the probability that the discriminator missclassifies $x^{\star}$ as safe program.  
%  \subsection{Multiple Input Block}
 
%  Our generator is diffirent than the original GAN, in the fact that generator doesn't just accept a single noise vector as input, but instead has  separate inputs, which correspond to . By manupulating each of these inputs independently we can change high level properties of attacks. 
 
 
 
 
 
 
 
 






\subsection{Train the discriminator:}
(a) take a a random real example x from the training dataset. (b) Get a new random noise vector z and using the generator network synthesize a fake example  $x^{\star}$. (c) Use the discriminator network to classify  $x$ and  $x^{\star}$. (d) Compute the classification error and backpropogate the total error to update it's trainable parameters, seeking to minimize it's classification error. 

\subsection{Train the generator}
For each training iteration
(a) Get a new random noise z and using the generator network, synthesize fake example $x^{\star}$. (b) Use discriminator network to classify $x^{\star}$. (c) Compute the classification error and backpropogate the error to update the generators trainable parameters, seeking to maximize the discriminator's error. 

GANs reaching Nash equilibrium when the following conditions are met generator produces fake examples that are not distinguishable from the real data in the training dataset. The discriminator can at best randomly. The discriminator can at best randomly guess whether a particular example is fake or real.  

 
\subsection{Adversarial Perceptron Learning With Dynamic Feature Transformation}
We now that many microarchitectural features are mutually correlated in different components of processors~\cite{PerSpectron}. This is due to the nature of components of pipeline, their functionality and interaction in an out of order superscaler process. For example, \textit{IcacheSquashes}, \textit{MiscStallCycles} and \textit{PendingTrapStallCycles} 
are mutually decorrelated in the fetch unit but they have high correlation with stalls 
and traps in other components, {\em i.e.} \textit{commit.NonSpecStalls}, 
\textit{lsq.thread0.rescheduledLoads} and \textit{dcache.blocked:no\_mshrs}. This property has been used in prior work~\cite{PerSpectron} to produce {\em replicated detectors} to improve the detection as the attack footprint moves in the pipeline.


Many considered random feature selection
strategy and showed that it can improve the security and robustness of forensic detectors and
standard ML-based to mitigate the adversarial machine learning attacks ~\cite{nowroozi2020survey, secureDetection2019}.
 In this work we use removal of mutually correlated features of different components of processor to reduce the transferability of classification parameters, and increase the security of our 
detection. 

Our weight clipping is very simple. We first group the microarchitectural features into mutually correlated sets {\em e.i.,} one feature per component of pipeline and uncore units. During training we add a layer between generator and discriminator. This layer chooses one feature from each group and sets the corresponding weight output values from the generator to zero.

Incredibly, this simple addition drastically improves accuracy on adversarial attacks, by ensuring that network doesn't become overdependant on certain units or groups of features that in effect just remember observation from training set. With weight clipping perceptron can not rely too much on any one units or features of the processor. Therefore, perceptron weight updates are more evenly distributed throughout the adversarial training. This makes perceptron learning much better at generalizing to unseen attacks, because the PerSpectron has been trained to produce accurate predictions even under unfamiliar conditions, such as those caused by dropping a highly correlated feature. At test time the weight clipping layer doesn't remove any weight so that the full Perceptron is used for prediction. 
