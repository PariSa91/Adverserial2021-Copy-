\section{ Overview}\label{alg}
\subsection{General Approach}
%\subsection{Game setup}
 The Generator's goal is to produce examples that capture the characteristics of the training dataset,  so that the samples it generates look indistinguishable from the training data. The generator can be thought of as a microarchitectural attack detection model in reverse. Microarchitectural attack detection model learns the patterns of suspicious and safe activity in a program to discern an attacks footprints. Instead of recognising the pattern, the Generator learns to create them essentially from scratch; indeed, the input into the Generator is often no more than a vector of random numbers. %this can go in introduction 
 
 
 The Generator learns through the feedback it receives from the discriminator's classification. The discriminator's goal is to determine whether a particular example is safe (coming from the training dataset) or suspicious (created by the Generator). 
Accordingly, each time the discriminator is fooled into classifying an adversarial attack as safe, the generator knows it did something well. Conversely each time the discriminator correctly rejects a generator-produced adversarial as attack, the generator receives the feedback that it needs to improve.
The discriminator continues to improve as well. Like any classifier, it learns from how far its predictions are from the true class (safe or suspicious). So, as the generator gets better at producing realistic adversarial sample, the discriminator gets better at telling adversarial samples from safe programs data, and both models continue to improve simultaneously.

We have a theoretical understanding of why the symmetric GAN training should converge to the Nash equilibrium. We noticed that our asymmetric GAN traning has a lot higher gradient and so trianing happens much more quickly at start. Although theoretically there is a chance that the training might not converge at all, we empirically show that with the right stopping criteria, the trained model performs better than symmetric GAN training. 
But our dreadful sacrifice leads to significant improvement in accuracy.

A conditional GAN allows us to direct the generator to synthesize the adversarial example we want. Although we could control the domain of example our GAN learned to emulate by our selection of the training dataset, we could not specify any of the characteristics of the data samples it is going to generate. GAN could synthesize realistic looking handwritten digits but we can not control what digits it produces.


\subsection{Adversarial Perceptron Learning With Feature Clipping}
We now that many microarchitectural features are mutually correlated in different components of processors~\cite{PerSpectron}. This is due to the nature of components of pipeline, their functionality and interaction in an out of order superscaler process. For example, \textit{IcacheSquashes}, \textit{MiscStallCycles} and \textit{PendingTrapStallCycles} 
are mutually decorrelated in the fetch unit but they have high correlation with stalls 
and traps in other components, {\em i.e.} \textit{commit.NonSpecStalls}, 
\textit{lsq.thread0.rescheduledLoads} and \textit{dcache.blocked:no\_mshrs}. This property has been used in prior work~\cite{PerSpectron} to produce {\em replicated detectors} to improve the detection as the attack footprint moves in the pipeline.


Many considered random feature selection
strategy and showed that it can improve the security and robustness of forensic detectors and
standard ML-based to mitigate the adversarial machine learning attacks ~\cite{nowroozi2020survey, secureDetection2019}.
 In this work we use removal of mutually correlated features of different components of processor to reduce the transferability of adversarial machine learning attacks, and increase the security of our 
detection. 

Our weight clipping is very simple. We first group the microarchitectural features into mutually correlated sets {\em e.i.,} one feature per component of pipeline and uncore units. During training we add a layer between generator and discriminator. This layer chooses one feature from each group and sets the corresponding weight output values from the generator to zero.

Incredibly, this simple addition drastically improves accuracy on adversarial attacks, by ensuring that network doesn't become overdependant on certain units or groups of features that in effect just remember observation from training set. With weight clipping perceptron can not rely too much on any one units or features of the processor. Therefore, perceptron weight updates are more evenly distributed throughout the adversarial training. This makes perceptron learning much better at generalizing to unseen attacks, because the PerSpectron has been trained to produce accurate predictions even under unfamiliar conditions, such as those caused by dropping a highly correlated feature. At test time the weight clipping layer doesn't remove any weight so that the full Perceptron is used for prediction. 



 \subsection{Training with Data Transmit And Recovery Channel Control}
 GANs are capable of producing examples ranging from simple handwritten digits to photo-realistic images of human faces. However, although we could control the domain of examples our GAN learned to emulate by our selection of the training dataset. we could not specify any of the characteristics of the data samples the gan would generate. We could not control whether it would produce, say a new sample of adversarial meltdown attack. In image recognition, this concern may seem trivial. Because if the goal is to generate number 9, you can just keep generating until you get the number you want. However, for a domain of microarchitectural attacks the domain of possible answers gets too large for such a brute-force solution to be practical. In simple GAN, you have no control on what category of sample input will get produced. There is no way to direct the Generator to synthesize say an attack or safe program sample. Let alone other features such as the type of covert channel such as Flush+Reload or Flush+Flush.
 
 Our design has the ability to decide what kind of adversarial microarchitectural attack will be generated. We could enter the descriptive features of attacks atomic tasks' samples into the generator and have it output a range of samples matching the criteria. It can greatly expedite the process of adversarial attack generation. We are sure there are many other practical applications where the ability to generate new attack sample that matches the channel type of our choice would a game changer. The CGAN was one of the first GAN innovations that made data generation possible. 
 
 CGAN is a generative adversarial learning whose generative and discriminator are conditioned during training by using some additional information e.g., labels. 
 The generator learns to produce realistic examples for each label in the training dataset and the discriminator learns to distinguish fake example-label pair from real-example pairs.   
 in contrast to a design where the Disriminator learns to assign a correct label to each real example in addition to distinguashing real example from fake, the discriminator does not learn to identify which class is which.  It learns to accept real matching pair while rejecting pairs that are mistmatched and pairs in which there are fake. 
 
 Accordingly in order to fool the discriminator it is not enough for the CGAN generator to produce realistic looking data. The example it generates also need to match their labels. After the generator is fully trained, this then allows us to specify what attack type we want the CGAN to synthesize by passing it the desired label. 
 
 The generator uses the noise vector and label to synthesize a fake example (x given that or conditioned on y). The goal of this fake example is to look in the eyes of Discriminator as close as possible to a real example for the given label.  
 
 
 
 The discriminator receives real examples with labels and fake examples with the label used to generate them. on the real example-label pairs, the discriminator learns how to recognize real data and how to recognize matching pairs. On the generator-based examples, it learns to recognize fake program-labeled pairs, thereby learning to tell them apart from the real ones. The discriminator outputs a single probability indicating its conviction that input is a real, matching pair. The discriminator's goal is to learn to reject all fake examples and all examples that fail to match their label, while accepting all real example-label pairs. 
 
 Note that for each fake example the same label is passed to both the generator and discriminator. Also note that the discriminator is never explicitly trained to reject mismatched pairs by being trained on real examples with mismatching labels; its ability to identify mismatched pairs is a by-product of being trained to accept only real matching pairs. 
 
 
 \subsection{Multiple Input Block}
 
 Our generator is diffirent than the original GAN, in the fact that generator doesn't just accept a single noise vector as input, but instead has  separate inputs, which correspond to . By manupulating each of these inputs independently we can change high level properties of attacks. 
 
 
 
 
 
 
 
 






\subsection{Train the discriminator:}
(a) take a a random real example x from the training dataset. (b) Get a new random noise vector z and using the generator network synthesize a fake example  $x^{\star}$. (c) Use the discriminator network to classify  $x$ and  $x^{\star}$. (d) Compute the classification error and backpropogate the total error to update it's trainable parameters, seeking to minimize it's classification error. 

\subsection{Train the generator}
For each training iteration
(a) Get a new random noise z and using the generator network, synthesize fake example $x^{\star}$. (b) Use discriminator network to classify $x^{\star}$. (c) Compute the classification error and backpropogate the error to update the generators trainable parameters, seeking to maximize the discriminator's error. 

GANs reaching Nash equilibrium when the following conditions are met generator produces fake examples that are not distinguishable from the real data in the training dataset. The discriminator can at best randomly. The discriminator can at best randomly guess whether a particular example is fake or real.  

 