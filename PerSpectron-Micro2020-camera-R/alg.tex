\section{ Overview}\label{alg}
\subsection{General Approach}
 
 GANs are capable of producing examples ranging from simple handwritten digits to photo-realistic images of human faces. However, although we could control the domain of examples our GAN learned to emulate by our selection of the training dataset. we could not specify any of the characteristics of the data samples the gan would generate. We could not control whether it would produce, say a new sample of adversarial meltdown attack. In image recognition, this concern may seem trivial.Because if the goal is to generate number 9, you can just keep generating until you get the number you want. However, for a domain of microarchitectural attacks the domain of possible answers gets too large for such a brute-force solution to be practical. In simple GAN, you have no control on what category of sample input will get produced. There is no way to direct the Generator to synthesize say an attack or safe program sample. Let alone other features such as the type of covert channel such as Flush+Reload or Flush+Flush.
 
 Our design has the ability to decide what kind of adversarial program will be generated. We could enter the descriptive features of attacks atomic tasks' samples into the generator and have it output a range of samples matching the criteria. It can greatly expedite the process of adversarial attack generation. We are sure there are many other practical applications where the ability to generate new attack sample that matches the channel type of our choice would a game changer. The CGAN was one of the first GAN innovations that made data generation possible. 
 
 CGAN is a generative adversarial learning whose generative and discriminator are conditioned during training by using some additional information e.g., labels. 
 The generator learns to produce realistic examples for each label in the training dataset and the discriminator learns to distinguish fake example-label pair from real-example pairs.   
 in contrast to a design where the Disriminator learns to assign a correct label to each real example in addition to distinguashing real example from fake, the discriminator does not learn to identify which class is which.  It learns to accept real matching pair while rejecting pairs that are mistmatched and pairs in which there are fake. 
 
 Accordingly in order to fool the discriminator it is not enough for the CGAN generator to produce realistic looking data. The example it generates also need to match their labels. After the generator is fully trained, this then allows us to specify what attack type we want the CGAN to synthesize by passing it the desired label. The generator uses the noise vector and label to synthesize a fake example (x given that or conditioned on y). The goal of this fake example is to look in the yes of Discriminator as close as possible to a real example for the given label.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 In more technical terms, the Generator's goal is to produce examples that capture the characteristics of the training dataset,  so that the samples it generates look indistinguishable from the training data. The generator can be thought of as a microarchitectural attack detection model in reverse. Microarchitectural attack detection model learns the patterns of suspicious and safe activity in a program to discern an attacks footprints. Instead of recognising the pattern, the Generator learns to create them essentially from scratch; indeed, the input into the Generator is often no more than a vector of random numbers. %this can go in introduction 
 
 
 The Generator learns through the feedback it receives from the discriminator's classification. The discriminator's goal is to determine whether a particular example is safe (coming from the training dataset) or suspicious (created by the Generator). 
Accordingly, each time the discriminator is fooled into classifying an adversarial attack as safe, the generator knows it did something well. Conversely each time the discriminator correctly rejects a generator-produced adversarial as attack, the generator receives the feedback that it needs to improve.
The discriminator continues to improve as well. Like any classifier, it learns from how far its predictions are from the true class (safe or suspicious). So, as the generator gets better at producing realistic adversarial sample, the discriminator gets better at telling adversarial samples from safe programs data, and both models continue to improve simultaneously. 

GANs are notoriously hard to both train and evaluate. As with any other cutting-edge field, opinions about what is the best approach are evolving and no study has been done on microarchitectural data. 

We want to use GANs to create a large dataset, but we need a large dataset to train the GAN in the first place. 
Our solution is as follows:
first we use standard data augmentation using program synthesising to create a larger dataset. Second, we used this dataset to train a GAN to create synthetic examples. Third, we use the augmented dataset from step 1 along with the GAN-produced synthetic examples from step 2 to train the PerSpectron. The GAN model we used is a variation of Deep Convolutions GAN. We had to adjust the dimensions of hidden layers and the dimensions of the output from the Generator and input into the Discriminator PerSpectron. Then we used the trained PerSpectron to classify unseen test set.  

Training dataset-- The dataset of all safe programs and real attacks that we want the generator to learn to emulate with near-perfect quality. This dataset serves as input($x$) to the generator network. 

Random noise vector-- The raw input ($z$) to the Generator network. This input is a vector of random numbers that the generator uses as a starting point for generating adversarial examples. 

Discriminator network-- The discriminator takes as input either the real examples ($x$) coming from the training set or an adversarial example $x^{\star}$ produced by the generator. For each example, the discriminator determines and outputs the probability of whether the example is adversarial.

Iterative training/tuning--For each of the discriminators predictions, we determine how good it is--much as we would for a regular classifier-- and use results to attractively tune the discriminator and the generator networks through backpropogation; the discriminator's weights and biases are updated to maximize its classification and accuracy ( maximizing the probability of correct prediction: ($x$) as attack and $x^{\star}$ is safe. 
The generator's weights and biases are updated to maximize the probability that the discriminator missclassifies $x^{\star}$ as safe program. 


We have a theoretical understanding of why the symmetric GAN training should converge to the Nash equilibrium. We noticed that our asymmetric GAN traning has a lot higher gradient and so trianing happens much more quickly at start. Although theoretically there is a chance that the training might not converge at all, we empirically show that with the right stopping criteria, the trained model performs better than symmetric GAN training. 
But our dreadful sacrifice leads to significant improvement in accuracy.


Accuracy on 3-fold cross validation improved by 7\%


We take an attack sample first and project it back to the latent space $z$.
We use the generator $G$ to generate a similar example to $x$, called ${x^{\star}}$ by $G(z)$. Then we use the classifier $C$ to classify the example $C(x^{\star})$, which generally already tends to misclassify way less than running the classification directly on $x$. 

Performance of the new classifier does drop on the training set, but accuracy improves on test set including adversarial attacks. Figure shows APL achieves significant improvement in classification accuracy as the number of training examples increases, especially synthetic examples produced by DCGAN. The dotted line depicts classification performance for classic data augmentation using program synthesizers. The performance improves as the quantity of new (augmented) training examples increases; however, the improvement plateaus around the accuracy of 80\%, beyond which additional examples fail to yield improvement. 

The dashed line shows the additional increase in accuracy achieved by augmenting the dataset using GAN-produced synthetic examples. Starting from the point beyond which additional classically augmented examples stopped improving accuracy, we added syntetic data generated by our DCGAN. The classification performance improved from 83\% to pver 90\%, demonstrating the usefulness of \scheme. 

Figure shows the loss value of generator vs training epoch for a range of features. Using a few high level features would result in discriminator losing the game to the generator faster. we define the new metric {\em Nash Equilibrium Speed} to be the loss of generative model over 1000 epochs. that is proportional to the vulnerability to adversarial machine learning attack and  inversely proportional  to the resiliency of a model to adversarial machine learning attacks. 

People have noted that adding labels to the dataâ€”that is, to break it up into categories, almost always improves the performance of GANs.

Generated data, showing high features in .. which is high in all the variations. 




% Our results show adversarial training does not perform
% as well when a different attack strategy {\em i.e.} new variations of Speculative attack is used by the attacker. 

\subsection{GAN training algorithm}

For each training iteration
\subsubsection{Train the discriminator:}
(a) take a a random real example x from the training dataset. (b) Get a new random noise vector z and using the generator network synthesize a fake example  $x^{\star}$. (c) Use the discriminator network to classify  $x$ and  $x^{\star}$. (d) Compute the classification error and backpropogate the total error to update it's trainable parameters, seeking to minimize it's classification error. 

\subsubsection{Train the generator}
For each training iteration
(a) Get a new random noise z and using the generator network, synthesize fake example $x^{\star}$. (b) Use discriminator network to classify $x^{\star}$. (c) Compute the classification error and backpropogate the error to update the generators trainable parameters, seeking to maximize the discriminator's error. 

GANs reaching Nash equilibrium when the following conditions are met generator produces fake examples that are not distinguishable from the real data in the training dataset. The discriminator can at best randomly. The discriminator can at best randomly guess whether a particular example is fake or real.  

A conditional GAN allows us to direct the generator to synthesize the adversarial example we want. Although we could control the domain of example our GAN learned to emulate by our selection of the training dataset, we could not specify any of the characteristics of the data samples it is going to generate. GAN could synthesize realistic looking handwritten digits but we can not control what digits it produces. 