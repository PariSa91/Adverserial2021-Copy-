\section{Introduction} \label{intro}
%In hardware we are limited to one layer neural network (Perceptron). Perceptron can only learn linearly separable functions.  
% showed that using carefully selected detailed microarchitectural features and a simple  single-layered perceptron can provide a readily implementable solution. 

The performance of modern processors is heavily dependent on prediction and speculative execution throughout the pipeline.  Speculative execution, however has recently shown itself to be a two edged sword with the disclosure in early 2018 of microarchitectural attacks such as Spectre and Meltdown~\cite{spectre, meltdown}. The total cost to secure each component individually against all potential microarchitectural attack variations incur increasingly high cost in real estate, performance and energy~\cite{canella2018systematic, schwarz2019zombieload}, giving us an unsatisfying choice between expensive software mitigations or turning off these performance features altogether.  

 
Several of the mitigation either impose high performance overhead
to the system and/or only mitigate a specific variant
~\cite{taram_csf19, intelanalysis, kiriansky2018dawg, domnitser2012non,saileshwar2019cleanupspec,retpoline, wang2007new, yu2019speculative, yan2018invisispec,CEASER,amd, koruyeh2019speccfi, arm_css}
Detection mechanisms can provide information to the system to trigger further
mitigations to avoid their overhead unless an attack is suspected~\cite{gulmezoglu2019fortuneteller, PerSpectron, cyclone2019}.
Prior works have detected microarchitectural attacks through two methods: Tracking unique cyclic properties in hardware ~\cite{cyclone2019} which can detect after the leakage completed, or detecting specific microarchitectural footprints of attacks using machine learning in hardware~\cite{PerSpectron, RHMD2017} and in software~\cite{gulmezoglu2019fortuneteller, cacheBasedDetection2016Chiappetta,
CloudRadar2016,
BlackHatFogh,
payer2016hexpads,
ICCAD2015Wang,
Duppel2013Zhang, 
mushtaq:cel-01824512}. 

More importantly, using machine learning enables detection before the attack can be successful, including detection of calibration phases required for attacks, training phases, etc. Contrary to static methods such as cyclic inference detection.  
However, this presents a particularly difficult
problem for simulation-to-real-world transfer: %besides the targeted vulnerable components of hardware changing in each variations of these attacks,
the detection
system must also handle {\em evasive attacks} which poses multiple challenges for designing a resilient detector, specially resiliency against {\em adversarial machine learning attacks}. There are three main categories of evasion techniques: (1) bandwidth reduction (2) polymorphic evasion and (3) adversarial machine learning attacks.   

 
Software detectors have high performance overhead due to sampling and are also prone to bandwidth evasion~\cite{Gaudiot2020, PerSpectron}. Bandwidth evasion is based on timing the completion of all attack atomic 
tasks to fit within the sampling interval. Recent studies~\cite{PerSpectron, cyclone2019} on detection of transient, MDS and cache attacks and malware~\cite{Malware2015, ensembleRaid2015,kazdagli-16,RHMD2017} addressed the software limitations by moving the detection of
attacks to hardware, allowing thousand-times higher sampling frequency (100ms vs 1$\mu$s) without incurring performance overhead. It would be very difficult for the attacker to implement an microarcitectural attack atomic phase requiring an interval under three microseconds. 

Detection in hardware does not only make evasion based on bandwidth reduction very hard, but it is also shown to make detection more robust against polymorphic evasions~\cite{PerSpectron, cyclone2019, RHMD2017}. Polymorphic attacks is when the attacker attempts to produce different binaries implementing the attack. Because in microarchitectural attacks, attacker can implement instructions that will never get itâ€™s result committed to the registers but force the CPU to execute instructions and leak data, it is important for security solutions 
to be able to detect attacks in the speculative execution feature 
space~\cite{wampler-19, PerSpectron}. 



Detection in hardware also allows the predictor to efficiently use and monitor a large set of informative microarchitectural features that is not . Recent study~\cite{PerSpectron} shows that low-level  features that are not limited to the committed instructions exist that are invariant under transformation and evasions by typical strategies
used by malware to evade signature based detectors~\cite{PaulKocher,paulKocherSpectreAttacks} and are accessible from hardware. 

The ability to use high number of features in hardware without incurring performance overhead makes adversarial machine learning attacks
described in prior hardware malware detector to be potentially prohibitively expensive against hardware detectors.
However, few studies have conclusively studied methods to increase robustness of  microarchitectural attack detectors in hardware against adversarial machine learning attacks. 


Designing such a system in hardware poses multiple challenges: First, often handcrafted adversarial attacks, and even real microarchitectural attack data are not sufficient for training and evaluation of a robust and reliable detector: While at least 350,000 new malware instances are being created and detected every day, microarchitectural attacks were reported only over 20 distinct attack variations in 2019. 
Previous detection mechanisms for microarchitectural attacks are trained and evaluated only on small samples, e.g., proof of concept code of current Spectre attacks and limited hand crafted adversarial perturbation similar to typical strategy 
used by malware to evade signature based
detectors~\cite{PaulKocher,paulKocherSpectreAttacks}. This is because no teams of human can realistically design enough adversarial training data for ML-based microarchitectural attack detectors. In this work we attempt to solve this problem. 



Second, augmenting the training dataset with adversarial examples~\cite{szegedy2014going, Goodfellow2015ADVexample, moosavidezfooli2016deepfool} has been proven to almost always improve the accuracy of machine learning detectors in other domains such as image classification. However,  projecting the adversarial perturbation to the program is not trivial in microarchitectural attacks. There is no direct mapping from microarchitectural features value to program instructions (software code). Thus, generating adversarial microarchitectural attack {\em manually} is  expensive and cumbersome, often requiring days for developing a single adversarial example that would fool a robust detector {\em i.e.,} to leak data before the detector flags it as suspicious. 

Adversarial machine learning examples can be generated {\em automatically}  or {\em manually}. Generating adversarial examples for microarchitectural attacks {\em automatically} has its own challenges.  
Unlike image pixels that can be easily manipulated without changing the image's visibility in the human eye, manipulating microarchitectural attacks is restricted by its functionality. 
Prior works in developing evasive malware~\cite{RHMD2017}, dynamically injected instructions to the malware binary to change the feature vector in a controlled way based on the reverse-engineered classifier to attempt to move the malware across the classification boundary. We will show in this work that data augmentation with examples produced by such conventional methods does not improve the accuracy of the microarchitectural attack classifier. 

Microarchitectural attacks' functionality depends profoundly on attacker microarchitecture being set by the attacker to a known state and the relative timing of instructions in the pipeline's fine-grain structures. 
Insertion of instructions should not interfere with attacks atomic tasks or otherwise can easily disrupt the state generated by attack channels and disable the attack. Yet when the attacker injects instructions carefully, {\em e.i.,} before and after attacks atomic tasks to ensure that the attack's functionality is not affected, we will see that the new microarchitectural samples do not add extra information to the model. This is because hardware detectors generalize based on the attack's footprint, which is observed only during the atomic tasks. Hardware microarchitectural attack detectors have 1000 times higher sampling frequency than software malware detectors. Hence, injected instructions similar to developing evasive malware can not hide the robust hardware detectors' footprint. Therefore,  techniques from evasive malware development are not sufficient for developing adversarial microarchitectural attacks. 

Forth,  
typical data augmentation technique using program synthesizers has many limitations to be used as adversarial example. For one, small modifications yields examples that do not diverge far from the original program. As a result, the additional examples do not add much variety to help the algorithm learn to generalize. In the case of microarchitectural attack detection, we want to see leakage happens using different technique, not just permutations of the same underlying attack.
In the case of microarchitectural attack detection, we want different examples of the same underlying vulnerability. Enriching a dataset with synthetic examples such as those produced by Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative}(described in section~\ref{overview}), has potential to further enrich the available data beyond traditional augmentation techniques. While the recent advances in deep generative models~\cite{goodfellow2014generative} have brought a rapid progress in image recognition~\cite{}, relatively no progress have been made in microarchitectural design models. We believe this is due in part to hardware constraints {\em e.g., area, power, latency and performance overhead}. GANs are usually made of Convolutions Neural Network (CNNs), CNNs are not easily amenable in hardware. In our knowledge, no method has been shown previously to train a hardware friendly ML-based model  That is precisely our first goal to investigate. 

 \samira {Add dynamic model transformation motive here }
 
 Thus, the following framework sets our motivations:
  We have a data set of known   microarchitectural attack performance counter samples collected in hardware. 
 We assume that the observations have been generated according to some unknown distribution. 
 We design an automatic model that tries to mimic observed attack and safe programs footprints. If we achieve this goal , we can use the generated samples to train our detector offline. Our goals are: (1) to generate new attack examples that appear to have been drawn from seen attacks and (2) to generate examples that are suitably different from the observed attacks. In other words, our model shouldn't simply reproduce things it has already seen. (3) A method to control the category of microarchitectural attacks generated {\em controlling the transmit and recovery channel} during training. (4) A dynamic training method to ensure that the model does not rely on specific features or unit of pipeline and to reduce the  transferability of the final classification parameter. 
 
 
 
Finally, the science of defenses for machine learning based  detection in hardware against evasive attacks are somewhat less well developed. This is the main reason for low confidence in ML-based detection systems in hardware against microarchitectural attacks. In this paper we contribute on several defensive goals:

\begin{enumerate}
\item We present the first use of adversarial learning to improve the
design of hardware detectors for microarchitectural attacks. 
\vspace{2mm}

\item We show that data augmentation using common program synthesizer does not improve the accuracy of detection, but training on adversarial examples significantly improves the accuracy of hardware detection for microarhitectural attacks. 

\item We design the first asymmetric  adversarial perceptron training, in which a simple one layer neural network that is amenable in hardware, can be trained  by a more complex deep neural network to detect adversarial machine learning attacks in hardware.
%\vspace{2mm}


\item We use the adversarial perceptron and a novel adaptive weight adjustment technique to produce \scheme{} which (1) significantly improves detection accuracy for broad range of microarchitectural attacks against evasive and adversarial machine learning attacks, and (2) it can be built in hardware with minimal performance and area overhead. 
\end{enumerate}
