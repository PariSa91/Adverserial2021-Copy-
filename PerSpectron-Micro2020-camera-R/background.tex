\section{Background and Motivation}\label{background}\label{motiv}

\subsection{Microarchitectural Attacks}

\subsection{Perceptron and Microarchitectural Features}

 


Neural network models used in software detectors, feature deep multi-layered  networks ({\em e.g.} RNN) are not easily amenable to hardware due to design and runtime complexity. But Perceptron learning has shown to be implementable in hardware for various  applications including branch prediction, prefetching, replacement policies, and CPUadaptation~\cite{intelISCA2019}. Recent microarchitectures from Oracle~\cite{SPARCT4}, AMD ({\em e.g.} Bobcat, Jaguar, Piledriver, Zen, etc.), and Samsung~\cite{Mongoose,M3} are documented as featuring perceptron-based branch predictors.


% Hinton~\cite{Hinton1985shape} 
% finds that using the entire space of possible features in the 
% training set made the 
% mapping from the features’ instantiation parameters to the
% object’s instantiation parameters became {\em linear} allowing the use of a simpler architecture, which could efficiently 
% handle more complex images.   Similarly 
In this work we laverage PerSpectron which is a  hardware detector for microarchitectural attacks which uses a simple one layer neural network,  allowing the use of a simpler architecture, which could efficiently 
handle more complex footprints of attacks.
PerSpectron~\cite{PerSpectron} found 106 microarchitectural features that when included in training, it could map a non-linearly separable problem to linear, which was then 
separable by {\em perceptron} and is readily implementable in hardware~\footnote{An example of such feature is the effect of misses and stalls 
in the Fetch stage. The squashed cycles in each stage, all the ROB, IQ, and 
Register full events, undone maps in the Rename stage, and memory order 
violation in the IEW stage propagate back to the Fetch stage. The relationship 
between these events' Fetch is not a simple cumulative function in an out-of-order 
processor. PerSpectron showed that because the formation about attacks moves around the
processor, mutually correlated features of all components of processors should be included in the feature set in order to detect new variations of attacks.However, features such as \textit{fetch.MiscStallCycle} capture the 
relationship.}. The result was competitive 
with a more complex deepNN that is not easily implementable in hardware.

% \subsection{Feature selection}
% Unlike image information that is simple pixels, microarhitectural features are much more complex. PerSpectron includes features that capture the relationship between multiple features. Therefore no hidden layer is needed~\footnote{}. The weights associated to these features have the potential to be updated further, similar to hidden weights in RNNs.
% No hidden layer was necessary---the 
% mapping from the features' instantiation parameters to the object's 
% instantiation parameters became linear.



\subsection{Adversarial Attacks Against Machine Learning}

Recent literature has considered two types of adversarial machine learning attacks: black-box and white-box attacks. Under the black-box attack model, the attacker does not have access to the classification model parameters; whereas in the white-box attack model, the attacker has complete access to the model architecture and parameters, including potential defense mechanisms.
%(Papernot et al., 2017; Tramer et al., 2017; ` Carlini & Wagner, 2017). 



White-box models assume that the attacker has complete knowledge of all the classifier parameters, i.e., network architecture and weights, as well as the details of any defense mechanism. Given an input x and its associated ground-truth label y, the attacker thus has access to the loss function J(x, y) used to train the network, and uses it to compute the adversarial perturbation $\delta$. Attacks can be targeted, in that they attempt to cause the perturbed attack to be misclassified as safe target class. 

As previously mentioned, black-box adversaries have no access to the classifier
or defense parameters. It is further assumed that they do not have access to a large training dataset
but can query the targeted DNN as a black-box, i.e., access labels produced by the classifier for
specific query images. The adversary trains a model, called substitute, which has a potentially
different architecture than the targeted classifier, using a very small dataset augmented by synthetic
data labeled by querying the classifier. Adversarial examples are then found by applying any
attack method on the substitute network. It was found that such examples designed to fool the substitute often end up being misclassified by the targeted classifier~\cite{szegedy2014going, papernot2017practical}. 

%In other words, black-box attacks are  transferrable from one model to the other. 

%\subsection{Handcrafting Adversarial Attack Strategies}

% The first technique is injection in which a malicious content is injected into benign process in order to avoid detection. 

% The downside of this technique is that the malicious Dynamic-Link Library (DLL) file must be stored on disk, which exposes it to detection by regular security solutions.

% To execute a malicious Dynamic-Link Library (DLL) under another process malware writes the path of a malicious DLL into a remote process’ address space. Then, to invoke the DLL’s execution, the malware creates a remote thread from the targeted process. This technique implies that the malicious DLL is stored on a disk before injecting it into the remote process. 



% To avoid storing the DLL on disk, Reflective DLL injection technique manually map the DLL’s raw binary into virtual memory, as the Windows loader would do, but without calling the Windows API’s LoadLibrary that might be detected by tools monitoring the LoadLibrary calls. It will be enough to get the correct address of the injected export function that will fully load and map remaining components of the DLL inside the target process, e.g. ReflectiveLoader().



% examples are executing DLL under another process, reflective DLL injection, hollowing the content of a benign process to include maliciois payload, 


%DLL injection is one of the simplest and most common processes injection techniques. To execute a malicious Dynamic-Link Library (DLL) under another process malware writes the path of a malicious DLL into a remote process’ address space. Then, to invoke the DLL’s execution, the malware creates a remote thread from the targeted process. This technique implies that the malicious DLL is stored on a disk before injecting it into the remote process.Steps for DLL injection:
%:Steps for DLL injection

% Locate the target process by traversing the running processes and call OpenProcess for obtaining a handle to it.
% Allocate the space for injecting the path of the malicious DLL file to the target process with a call to VirtualAllocEx with the targeted process handle.
% Write the path of the DLL into the allocated space with WriteProcessMemory.
% Retrieve the address of LoadLibrary from kernel32.dll, that given the path to DLL, loads it into memory (does not execute it though).
% Call CreateRemoteThread passing it the address of LoadLibrary causing the injected DLL file’s path to be loaded into memory and executed.
% The downside of this technique is that the malicious DLL file must be stored on disk, which exposes it to detection by regular security solutions. Nevertheless, this technique is employed by malware developers and is widespread in the wild. For example, Poison Ivy, a popular and long-standing RAT, uses DLL injection. Poison Ivy has been involved in several APT campaigns recommending itself as a tool of choice by APT groups for espionage operations.
%https://www.deepinstinct.com/2019/09/15/malware-evasion-techniques-part-1-process-injection-and-manipulation/#:~:text=Process%20injection%20and%20manipulation%20is%20a%20prominent%20method,undetected%20and%20launch%20and%20execute%20additional%20successful%20attacks.

\subsection{Defenses}

 Hardware Malware Detectors (HMDs)~\cite{RHMD2017} proposed defenses against the proliferation of malware. They stochastically switch between different detectors. These detectors can be shown to be provably more difficult to
reverse engineer based on resent results in probably approximately
correct (PAC) learnability theory. 



Defensive distillation~\cite{papernot2016distillation} trains the classifier in two rounds using a variant of the
distillation~\cite{hinton2015distilling} method. This has the desirable effect of learning a smoother network
and reducing the amplitude of gradients around input points, making it difficult for attackers to
generate adversarial examples~\cite{papernot2016distillation}. It was, however, shown that, while defensive
distillation is effective against white-box attacks, it fails to adequately protect against black-box
attacks transferred from other networks~\cite{Carlini2017}.

A popular approach to defend against adversarial machine attack is to augment the training dataset with adversarial examples~\cite{szegedy2014going, Goodfellow2015ADVexample, moosavidezfooli2016deepfool}. Adversarial examples are generated using one or more chosen attack models and added to the training
set. This often results in increased robustness when the attack model used to generate the augmented
training set is the same as that used by the attacker. It tends to make the model more robust to white-box attacks than to black-box attacks due to gradient masking~\cite{Papernot2016TowardsTS, tramer2020ensemble}.



