\section{Background and Motivation}\label{background}
\label{motiv}
\subsection{Microarchitectural Attacks}
The following two subsections explain the attacks considered in our work. 
We categorize them as (1) cache side-channel attacks, which exploit 
timing differences caused by the lower latency of CPU caches 
(compared to physical memory), and (2) speculative attacks, 
which exploit microarchitectural vulnerabilities and leak the data 
using the above-described side-channel attacks. 

In particular, for cache side-channel attacks, we examine Prime+Probe, 
Flush+Reload, and Flush+Flush attacks. For speculative (also known as 
transient execution) attacks, Spectre Variant1 (SpectreV1), Meltdown, 
and SpectreRSB. Our training set is limited to these six attacks.\footnote{For 
practical reasons, our data set does not include all known microarchitectural 
attacks  ({\em e.g}, some variants of speculation attacks, side channels on 
other CPU structures, and RowHammer attacks); however, we believe 
PerSpectron can be trained to detect additional attacks.}

We then describe the perceptron, a small, single-layered learning network
that is cheaply implementable in hardware and is the foundational computational 
structure of our work.

\subsection{Cache attacks}
In a Prime+Probe attack~\cite{PrimeProbe2015last}, the attacker primes the cache 
by filling cache sets with its own code or data, then waits for a set time while 
the victim executes. Finally, the attacker probes the cache by executing and 
measuring the time to load each set of the primed data or code. If the victim has 
accessed some cache sets, it will have evicted some of the attacker's cache 
lines, which the attacker observes via increased access latencies for those lines.

In a Flush+Reload attack~\cite{FlushReload2014Yarom}, the attacker and the 
victim share some pages in their address space, {\em e.g.} via shared libraries. 
First, the monitored data is flushed from the cache hierarchy. Second, 
the attacker waits to allow the victim time to access this data. Third, 
the attacker reloads the memory line, measuring the time to load it. If during 
the waiting phase the victim accesses the data, the line will be present in 
the cache and the reload operation will take a short time, exposing the 
victim's access behavior. If the victim has not accessed the memory 
line, the attacker will observe a longer access time. 

Unlike the above attacks, a Flush+Flush attack ~\cite{GrussFlushFlush} does 
not make any memory accesses and causes no cache misses from the attacker process. 
Instead, this attack relies on the execution time of the flush instruction: 
if an address is present in the cache, flushing takes more time. 
Flush+Flush attacks are stealthy~\cite{GrussFlushFlush}, {\em i.e.} the spy process 
cannot be detected based on cache hits and misses detection 
mechanisms\cite{GrussFlushFlush}. Because Flush+Flush conducts neither
caches accesses nor causes misses, it cannot be detected by Cyclone~\cite{cyclone2019},
a state-of-the-art side channel detection solution.  

\subsection{Speculative attacks}
\textit{Spectre Variant1}~\cite{spectre}, or SpectreV1, exploits branch predictors 
used to predict the direction of conditional branches. The attacker mis-trains 
the branch predictor to bypass the boundary check of a data structure and 
speculatively access secret data out of bounds. As a result, the CPU 
speculatively brings indexing data into the cache that would not have 
been loaded otherwise. The attacker uses the secret data to index 
into its own address space and uses a cache side-channel (such as 
Flush+Reload) to leak the data. 

The \textit{Meltdown} attack~\cite{meltdown} relies on the fact that permission
checks are performed late in the execution pipeline, and a fault is generated only 
for a committed instruction. From the time an instruction is marked for permission
exception at the beginning of the pipeline, to the time that an exception can be raised, 
if the attacker makes a (normally unauthorized) memory access, the secret data can be 
cached and leaked using a side-channel.

\textit{SpectreRSB}~\cite{koruyeh2018spectre} is another Spectre attack variant
that exploits the return stack buffer (RSB), a predictor unit to predict the target of 
return instructions. In this attack, the attacker pollutes the RSB with an unmatched 
call/return pair, which redirects the speculative control flow of the program to 
a malicious Spectre-like setup.
% \subsection{Perceptron and Microarchitectural Features}

 


% Neural network models used in software detectors, feature deep multi-layered  networks ({\em e.g.} RNN) are not easily amenable to hardware due to design and runtime complexity. But Perceptron learning has shown to be implementable in hardware for various  applications including branch prediction, prefetching, replacement policies, and CPUadaptation~\cite{intelISCA2019}. Recent microarchitectures from Oracle~\cite{SPARCT4}, AMD ({\em e.g.} Bobcat, Jaguar, Piledriver, Zen, etc.), and Samsung~\cite{Mongoose,M3} are documented as featuring perceptron-based branch predictors.


% % Hinton~\cite{Hinton1985shape} 
% % finds that using the entire space of possible features in the 
% % training set made the 
% % mapping from the features’ instantiation parameters to the
% % object’s instantiation parameters became {\em linear} allowing the use of a simpler architecture, which could efficiently 
% % handle more complex images.   Similarly 
% In this work we laverage PerSpectron which is a  hardware detector for microarchitectural attacks which uses a simple one layer neural network,  allowing the use of a simpler architecture, which could efficiently 
% handle more complex footprints of attacks.
% PerSpectron~\cite{PerSpectron} found 106 microarchitectural features that when included in training, it could map a non-linearly separable problem to linear, which was then 
% separable by {\em perceptron} and is readily implementable in hardware~\footnote{An example of such feature is the effect of misses and stalls 
% in the Fetch stage. The squashed cycles in each stage, all the ROB, IQ, and 
% Register full events, undone maps in the Rename stage, and memory order 
% violation in the IEW stage propagate back to the Fetch stage. The relationship 
% between these events' Fetch is not a simple cumulative function in an out-of-order 
% processor. }. 
% PerSpectron showed that because the formation about attacks moves around the
% processor, mutually correlated features of all components of processors should be included in the feature set in order to detect new variations of attacks.However, features such as \textit{fetch.MiscStallCycle} capture the 
% relationship.
% The result was competitive 
% with a more complex deepNN that is not easily implementable in hardware.

% \subsection{Feature selection}
% Unlike image information that is simple pixels, microarhitectural features are much more complex. PerSpectron includes features that capture the relationship between multiple features. Therefore no hidden layer is needed~\footnote{}. The weights associated to these features have the potential to be updated further, similar to hidden weights in RNNs.
% No hidden layer was necessary---the 
% mapping from the features' instantiation parameters to the object's 
% instantiation parameters became linear.



\subsection{Adversarial Attacks Against Machine Learning}

Recent literature has considered two types of adversarial machine learning attacks: black-box and white-box attacks. Under the black-box attack model, the attacker does not have access to the classification model parameters; whereas in the white-box attack model, the attacker has complete access to the model architecture and parameters, including potential defense mechanisms.
%(Papernot et al., 2017; Tramer et al., 2017; ` Carlini & Wagner, 2017). 



White-box models assume that the attacker has complete knowledge of all the classifier parameters, i.e., network architecture and weights, as well as the details of any defense mechanism. Given an input x and its associated ground-truth label y, the attacker thus has access to the loss function J(x, y) used to train the network, and uses it to compute the adversarial perturbation $\delta$. Attacks can be targeted, in that they attempt to cause the perturbed attack to be misclassified as safe target class. 

As previously mentioned, black-box adversaries have no access to the classifier
or defense parameters. It is further assumed that they do not have access to a large training dataset
but can query the targeted DNN as a black-box, i.e., access labels produced by the classifier for
specific query images. The adversary trains a model, called substitute, which has a potentially
different architecture than the targeted classifier, using a very small dataset augmented by synthetic
data labeled by querying the classifier. Adversarial examples are then found by applying any
attack method on the substitute network. It was found that such examples designed to fool the substitute often end up being misclassified by the targeted classifier~\cite{szegedy2014going, papernot2017practical}. 

%In other words, black-box attacks are  transferrable from one model to the other. 

%\subsection{Handcrafting Adversarial Attack Strategies}

% The first technique is injection in which a malicious content is injected into benign process in order to avoid detection. 

% The downside of this technique is that the malicious Dynamic-Link Library (DLL) file must be stored on disk, which exposes it to detection by regular security solutions.

% To execute a malicious Dynamic-Link Library (DLL) under another process malware writes the path of a malicious DLL into a remote process’ address space. Then, to invoke the DLL’s execution, the malware creates a remote thread from the targeted process. This technique implies that the malicious DLL is stored on a disk before injecting it into the remote process. 



% To avoid storing the DLL on disk, Reflective DLL injection technique manually map the DLL’s raw binary into virtual memory, as the Windows loader would do, but without calling the Windows API’s LoadLibrary that might be detected by tools monitoring the LoadLibrary calls. It will be enough to get the correct address of the injected export function that will fully load and map remaining components of the DLL inside the target process, e.g. ReflectiveLoader().



% examples are executing DLL under another process, reflective DLL injection, hollowing the content of a benign process to include maliciois payload, 


%DLL injection is one of the simplest and most common processes injection techniques. To execute a malicious Dynamic-Link Library (DLL) under another process malware writes the path of a malicious DLL into a remote process’ address space. Then, to invoke the DLL’s execution, the malware creates a remote thread from the targeted process. This technique implies that the malicious DLL is stored on a disk before injecting it into the remote process.Steps for DLL injection:
%:Steps for DLL injection

% Locate the target process by traversing the running processes and call OpenProcess for obtaining a handle to it.
% Allocate the space for injecting the path of the malicious DLL file to the target process with a call to VirtualAllocEx with the targeted process handle.
% Write the path of the DLL into the allocated space with WriteProcessMemory.
% Retrieve the address of LoadLibrary from kernel32.dll, that given the path to DLL, loads it into memory (does not execute it though).
% Call CreateRemoteThread passing it the address of LoadLibrary causing the injected DLL file’s path to be loaded into memory and executed.
% The downside of this technique is that the malicious DLL file must be stored on disk, which exposes it to detection by regular security solutions. Nevertheless, this technique is employed by malware developers and is widespread in the wild. For example, Poison Ivy, a popular and long-standing RAT, uses DLL injection. Poison Ivy has been involved in several APT campaigns recommending itself as a tool of choice by APT groups for espionage operations.
%https://www.deepinstinct.com/2019/09/15/malware-evasion-techniques-part-1-process-injection-and-manipulation/#:~:text=Process%20injection%20and%20manipulation%20is%20a%20prominent%20method,undetected%20and%20launch%20and%20execute%20additional%20successful%20attacks.

\subsection{Defenses}

 Hardware Malware Detectors (HMDs)~\cite{RHMD2017} proposed defenses against the proliferation of malware. They stochastically switch between different detectors. These detectors can be shown to be provably more difficult to
reverse engineer based on resent results in probably approximately
correct (PAC) learnability theory. 



Defensive distillation~\cite{papernot2016distillation} trains the classifier in two rounds using a variant of the
distillation~\cite{hinton2015distilling} method. This has the desirable effect of learning a smoother network
and reducing the amplitude of gradients around input points, making it difficult for attackers to
generate adversarial examples~\cite{papernot2016distillation}. It was, however, shown that, while defensive
distillation is effective against white-box attacks, it fails to adequately protect against black-box
attacks transferred from other networks~\cite{Carlini2017}.

A popular approach to defend against adversarial machine attack is to augment the training dataset with adversarial examples~\cite{szegedy2014going, Goodfellow2015ADVexample, moosavidezfooli2016deepfool}. Adversarial examples are generated using one or more chosen attack models and added to the training
set. This often results in increased robustness when the attack model used to generate the augmented
training set is the same as that used by the attacker. It tends to make the model more robust to white-box attacks than to black-box attacks due to gradient masking~\cite{Papernot2016TowardsTS, tramer2020ensemble}.


\subsection{Perceptron-based Microarchitectural Attack Detector}
A perceptron is a vector of weights that records correlations between an input
vector and a target value. It can be used to classify inputs into one of two
classes~\cite{RosenblattPerceptron}. We construct a quantized desired response $d(n)$ to a perceptron:
\[
  d(n)=\begin{cases}
    +1, & \text{if $x(n)$ belongs to malicious class}.\\
    -1, & \text{if $x(n)$  belongs to benign class}.
  \end{cases}
\]
The perceptron computes the weighted sum of the input patterns $x(n)$ 
and compares it to a threshold value.
If the sum exceeds the threshold, the output of the perceptron is +1, otherwise, is it -1. 
The perceptron weights are updated after the desired outcome $d(n)$ of the predicted
event is known. If the prediction was correct then the weights remain unchanged. 
Otherwise, the inputs are used to update the corresponding weights:\\
$w(n+1)=w(n)+\mu[d(n)-y(n)]x(n)$
where $\mu$ is the learning-rate parameter and the difference $d(n)-y(n)$ is the 
error signal.

Perceptrons are a natural choice for building a microarchitectural attack
detector because they can be efficiently implemented in hardware. Other forms
of neural networks such as those trained by back-propagation, and other forms
of machine learning such as decision trees, are less attractive because of
excessive implementation costs. 