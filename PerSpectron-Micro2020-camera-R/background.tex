\section{Background and Motivation}\label{background}
%summary of background


% \subsection{Evasive Attacks}
% It is natural to expect that an attacker would attempt to change their attack to avoid 
% detection using evasive attacks. 

% Bandwidth reduction mimicry is a common evasion strategy to slow down the rate of the 
% attack to weaken the microarchitectural signal. 
% Bandwidth evasion fits an attack atomic phase within a sampling window. 
% Software-only detection methods are also prone to bandwidth evasion. Software detectors have sampling intervals around 100 milliseconds, while Spectre requires 65 microseconds. Hardware detectors have much shorter intervals, in the range of 3 microseconds. A hardware detector could sample a Spectre phase 20 times, making it difficult to evade~\cite{PerSpectron}. Thus, bandwidth evasion is a problem with software detectors since sampling frequency is limited due to performance overhead in software. While in hardware, sampling interval can be decreased under fastest attacks atomic task length, with minimal performance overhead. 




% Bandwidth evasion is based on timing the completion of all attack atomic 
% tasks to fit within the sampling interval. It is a problem with the 
% low sampling frequency of the software detector. Li and 
% Gaudiot~\cite{Gaudiot2020} demonstrated that performance counter-based 
% detectors can be evaded by changing the bandwidth of the attack so that 
% it runs inside the 100 ms sampling interval of the detector. They identified 
% three "Atomic Tasks" that, if interrupted, will disable the attack: 
% (1) Flushing cache lines - 10 $\mu$s
% (2) Mistraining branch predictor - 13 $\mu$s
% (3) Attempting to infer the secret byte that is loaded into cache - 38 $\mu$s. 
% The authors concluded that putting the attack to sleep after all three tasks were 
% completed is the optimum evasion strategy for an attack.  This means that the 
% attack runs for 61 $\mu$s before being put to sleep, which allows Spectre to run 
% inside the 100 ms sampling interval of the detector. Our sampling interval is 
% 3 $\mu$s, which gives 20 sampling intervals within the 61 $\mu$s run time it takes 
% to complete all three tasks, making PerSpectron resistant to this evasion strategy.  
% The authors acknowledged that future work should be done on a dedicated hardware 
% detector to reduce performance overhead.
% Therefore evasion is made more difficult by decreasing the sampling 
% interval to below the run time of essential tasks of the attack.

% Polymorphic evasion is when the attacker attempts to produce different binaries implementing the attack. Examples of typical strategy
% used are: moving the leak to a function that cannot be inline or check the bounds with an \texttt{AND} mask, rather than \texttt{<}.
% Recent study shows that PerSpectron is resilient against typical strategy
% used by malware to evade signature based 
% detectors~\cite{PaulKocher,paulKocherSpectreAttacks}. However, they have not attempted to conclusively prove that the design cannot be evaded by adversarial ML attacks.

\subsection{Microarchitectural Attacks}

\subsection{Adversarial Attacks Against Machine Learning}

Recent literature has considered two types of threat models: black-box and white-box attacks. Under the black-box attack model, the attacker does not have access to the classification model parameters; whereas in the white-box attack model, the attacker has complete access to the model architecture and parameters, including potential defense mechanisms.
%(Papernot et al., 2017; Tramer et al., 2017; ` Carlini & Wagner, 2017). 




As previously mentioned, black-box adversaries have no access to the classifier
or defense parameters. It is further assumed that they do not have access to a large training dataset
but can query the targeted DNN as a black-box, i.e., access labels produced by the classifier for
specific query images. The adversary trains a model, called substitute, which has a potentially
different architecture than the targeted classifier, using a very small dataset augmented by synthetic
data labeled by querying the classifier. Adversarial examples are then found by applying any
attack method on the substitute network. It was found that such examples designed to fool the substitute often end up being misclassified by the targeted classifier~\cite{szegedy2014going, papernot2017practical}. In other words, black-box attacks are  transferrable from one model to the other. 

White-box models assume that the attacker has complete knowledge of all the classifier parameters, i.e., network architecture and weights, as well as the details of any defense mechanism. Given an input x and its associated ground-truth label y, the attacker thus has access to the loss function J(x, y) used to train the network, and uses it to compute the adversarial perturbation $\delta$. Attacks can be targeted, in that they attempt to cause the perturbed attack to be misclassified as safe target class. 

Unlike image classification there is no direct mapping from microarchitectural features value to software code.   
Projecting the adversarial perturbation $\delta$ to the program transformation is not trivial for both white-box and black-box attack model. 
%For example the number of \textit{commit.NonSpecStalls} can be 
Thus, generating adversarial microarchitectural attack manually is  expensive and cumbersome, often requiring days for developing a single adversarial example that would fool a robust detector {\em i.e.,} to leak data before the detector flags it as suspicious. 

Often handcrafted adversarial attacks, and even real microarchitectural attack data are not sufficient for training and evaluation of a robust and reliable detector: While at least 350,000 new malware instances are being created and detected every day, microarchitectural attacks were reported only over 20 distinct attack variations. 
Previous detection mechanisms for microarchitectural attacks are trained and evaluated only on small samples, e.g., proof of concept code of current Spectre attacks and limited hand crafted adversarial perturbation similar to typical strategy 
used by malware to evade signature based 
detectors~\cite{PaulKocher,paulKocherSpectreAttacks}. No teams of human can realistically design enough adversarial training data for ML-based microarchitectural attack detectors.
This is causing low confidence in AI-based defense systems against microarchitectural attacks.

Typical data augmentation technique using program synthesizers has many limitations. For one, small modifications yields examples that do not diverge far from the original program. As a result, the additional examples do not add much variety to help the algorithm learn to generalize. In the case of microarchitectural attack detection, we want to see leakage happens using different technique, not just permutations of the same underlying attack.
In the case of microarchitectural attack detection, we want different examples of the same underlying vulnerability. Enriching a dataset with synthetic examples such as those produced by GANs, has potential to further enrich the available data beyond traditional augmentation techniques. That is precisely our first goal to investigate. 

 
 The following framework sets our motivations:
  We have a data set of known attack samples $X$. 
 We assume that the observations have been generated according to some unknown distribution. 
 A generative model tries to mimic observed attack probability. If we achieve this goal , we can sample from the generated probability to generate observations that appear to have been drawn from known attacks. Our goal are: (1) to generate new attack examples that appear to have been drawn from seen attacks and (2) to generate examples that are suitably different from the observation in $X$. In other words, our model shouldn't simply reproduce things it has already seen. 


%\subsection{Handcrafting Adversarial Attack Strategies}

% The first technique is injection in which a malicious content is injected into benign process in order to avoid detection. 

% The downside of this technique is that the malicious Dynamic-Link Library (DLL) file must be stored on disk, which exposes it to detection by regular security solutions.

% To execute a malicious Dynamic-Link Library (DLL) under another process malware writes the path of a malicious DLL into a remote process’ address space. Then, to invoke the DLL’s execution, the malware creates a remote thread from the targeted process. This technique implies that the malicious DLL is stored on a disk before injecting it into the remote process. 



% To avoid storing the DLL on disk, Reflective DLL injection technique manually map the DLL’s raw binary into virtual memory, as the Windows loader would do, but without calling the Windows API’s LoadLibrary that might be detected by tools monitoring the LoadLibrary calls. It will be enough to get the correct address of the injected export function that will fully load and map remaining components of the DLL inside the target process, e.g. ReflectiveLoader().



% examples are executing DLL under another process, reflective DLL injection, hollowing the content of a benign process to include maliciois payload, 


%DLL injection is one of the simplest and most common processes injection techniques. To execute a malicious Dynamic-Link Library (DLL) under another process malware writes the path of a malicious DLL into a remote process’ address space. Then, to invoke the DLL’s execution, the malware creates a remote thread from the targeted process. This technique implies that the malicious DLL is stored on a disk before injecting it into the remote process.Steps for DLL injection:
%:Steps for DLL injection

% Locate the target process by traversing the running processes and call OpenProcess for obtaining a handle to it.
% Allocate the space for injecting the path of the malicious DLL file to the target process with a call to VirtualAllocEx with the targeted process handle.
% Write the path of the DLL into the allocated space with WriteProcessMemory.
% Retrieve the address of LoadLibrary from kernel32.dll, that given the path to DLL, loads it into memory (does not execute it though).
% Call CreateRemoteThread passing it the address of LoadLibrary causing the injected DLL file’s path to be loaded into memory and executed.
% The downside of this technique is that the malicious DLL file must be stored on disk, which exposes it to detection by regular security solutions. Nevertheless, this technique is employed by malware developers and is widespread in the wild. For example, Poison Ivy, a popular and long-standing RAT, uses DLL injection. Poison Ivy has been involved in several APT campaigns recommending itself as a tool of choice by APT groups for espionage operations.
%https://www.deepinstinct.com/2019/09/15/malware-evasion-techniques-part-1-process-injection-and-manipulation/#:~:text=Process%20injection%20and%20manipulation%20is%20a%20prominent%20method,undetected%20and%20launch%20and%20execute%20additional%20successful%20attacks.
 

\subsection{Defenses}
A popular approach to defend against adversarial attack in computer vision is to augment the training dataset with adversarial examples~\cite{szegedy2014going, Goodfellow2015ADVexample, moosavidezfooli2016deepfool}. Adversarial examples are generated using one or more chosen attack models and added to the training
set. This often results in increased robustness when the attack model used to generate the augmented
training set is the same as that used by the attacker. It tends to make the model more robust to white-box attacks than to black-box attacks due to gradient masking~\cite{Papernot2016TowardsTS, tramer2020ensemble}.

%2.2.2 DEFENSIVE DISTILLATION
Defensive distillation~\cite{papernot2016distillation} trains the classifier in two rounds using a variant of the
distillation~\cite{hinton2015distilling} method. This has the desirable effect of learning a smoother network
and reducing the amplitude of gradients around input points, making it difficult for attackers to
generate adversarial examples~\cite{papernot2016distillation}. It was, however, shown that, while defensive
distillation is effective against white-box attacks, it fails to adequately protect against black-box
attacks transferred from other networks~\cite{Carlini2017}.

Defenses against the proliferation of malware include Hardware Malware Detectors (HMDs)~\cite{RHMD2017}. They stochastically switch between different detectors. These detectors can be shown to be provably more difficult to
reverse engineer based on resent results in probably approximately
correct (PAC) learnability theory. 
%RHMD studied limited number of high level features and techniques used for attacking malware detectors. 


\subsection{Perceptron}
Unlike image information that is simple pixels, microarhitectural features are much more complex.PerSpectron includes features that capture the relationship between multiple features. Therefore no hidden layer is needed~\footnote{An example of such feature is the effect of misses and stalls 
in the Fetch stage. The squashed cycles in each stage, all the ROB, IQ, and 
Register full events, undone maps in the Rename stage, and memory order 
violation in the IEW stage propagate back to the Fetch stage. The relationship 
between these events' Fetch is not a simple cumulative function in an out-of-order 
processor. However, features such as \textit{fetch.MiscStallCycle} capture the 
relationship.}. The weights associated to these features have the potential to be updated further, similar to hidden weights in RNNs.
No hidden layer was necessary---the 
mapping from the features' instantiation parameters to the object's 
instantiation parameters became linear. 

In addition, PerSpectron showed that because the formation about attacks moves around the
processor, mutually correlated features of all components of processors should be included in the feature set in order to detect new variations of attacks. 
% Neural network models used     in current work feature deep multi-layered  networks ({\em e.g.} RNN)  are not easily amenable to hardware due to design and runtime complexity. 
% Hinton~\cite{Hinton1985shape} 
% finds that using the entire space of possible instantiation parameters in the 
% training set allowed the use of a simpler architecture, which could efficiently 
% handle more complex images.  Similarly PerSpectron showed that using carefully selected detailed microarchitectural features and a simple  single-layered perceptron can provide a readily implementable solution~\cite{PerSpectron}. 
% Perceptron learning has shown to be implementable in hardware for various  applications including branch prediction, prefetching, replacement policies, and CPUadaptation~\cite{intelISCA2019}. Recent microarchitectures from Oracle~\cite{SPARCT4}, AMD ({\em e.g.} Bobcat, Jaguar, Piledriver, Zen, etc.), and Samsung~\cite{Mongoose,M3} are documented as featuring perceptron-based branch predictors.

% Unlike image information that is simple pixels, microarhitectural features are much more complex.PerSpectron includes features that capture the relationship between multiple features. That's why hidden layer is not needed. ~\footnote{An example of such feature is the effect of misses and stalls 
% in the Fetch stage. The squashed cycles in each stage, all the ROB, IQ, and 
% Register full events, undone maps in the Rename stage, and memory order 
% violation in the IEW stage propagate back to the Fetch stage. The relationship 
% between these events' Fetch is not a simple cumulative function in an out-of-order 
% processor. However, features such as \textit{fetch.MiscStallCycle} capture the 
% relationship.} The weights associated to these features have the potential to be updated further, similar to hidden weights in RNNs.
% No hidden layer was necessary---the 
% mapping from the features' instantiation parameters to the object's 
% instantiation parameters became linear. In addition, PerSpectron showed that as the formation about attacks moves around the
% processor, mutually correlated features of all components of processors should be included in the feature set in order to detect new variations of attacks. 




 \subsection{Game setup}