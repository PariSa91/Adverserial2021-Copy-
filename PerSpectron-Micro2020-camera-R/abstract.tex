\begin{abstract}
Microarchitectural attack detection in hardware is attracting
more and more research interests.  It has plenty of general advantages over software detection, including minimal performance overhead, access to a larger set of detailed microarchitectural features and  higher robustness to bandwidth mimicry evasion. Use of machine learning in hardware enables detection before leakage which is critical. However,  the science of defenses against adversarial machine learning attacks are somewhat less
well developed. This is the main reason for low confidence in
ML-based detection systems in hardware against microarchitectural attacks.

Training a classification model on augmenting adversarial examples has shown to almost always improve the accuracy of the classifier. We show the challenges of adversarial example generation against  microarchitectural attack detection system.
Most adversarial example generation strategies in malware are based on developing an dynamic system that extrapolates
appropriate perturbations from benign data and adds them back onto those data based on the reverse engineered model, to turn into adversarial
examples.  As a consequence of
perturbing from existing malware, the generated attacks resemble their unperturbed version, which is
not rigorously proved to fool hardware detectors yet obviously limit the adversarial diversity. We show that augmenting those examples in training does not improve the detector resiliency. We propose {\scheme} a novel technique based on generative adversarial network theory~\cite{goodfellow2014generative} by training a perceptron based detector by a Deep Neural Network as a generator in software.  Our final design is amenable to hardware. 

%Traditional  approaches can be roughly split into to two classes, i


\end{abstract}